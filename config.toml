# =============================================================================
# VLLM Qwen2.5-Omni System Configuration
# =============================================================================

[model]
name = "Qwen/Qwen2.5-Omni-7B-AWQ"
model_path = "./models/Qwen2.5-Omni-7B-AWQ"
trust_remote_code = true
dtype = "auto"
max_model_len = 32768

[hardware]
gpu_memory_utilization = 0.9
tensor_parallel_size = 1
vllm_engine = "v1"  # "v0" | "v1" - VLLM engine version


[generation]
temperature = 0.0
max_tokens = 512
top_p = 0.85

[paths]
input_dir = "./videos"
output_dir = "./captions/videos"
output_formats = ["txt", "csv", "json"]
# output_format = "csv"  - save in one format only

[processing]
mode = "video"  # "video" | "image" | "mixed"
use_audio_in_video = false
overwrite_existing = true # overwrite caption files if already exist
fps = 16.0
batch_size = 4  # Number of media files to process simultaneously
batch_mode = true  # Enable batch processing for better performance
save_conversations = true  # Save detailed conversation logs for each file
max_num_batched_tokens = 16384  # V1 optimization: token budget per step

# Extensions based on mode
video_extensions = [".mp4", ".avi", ".mov", ".mkv", ".webm"]
image_extensions = [".jpg", ".jpeg", ".png", ".webp", ".bmp"]

[conversation]
enable_multi_round = true

[files]
prompts_config = "prompts.toml"  # Path to prompts configuration file